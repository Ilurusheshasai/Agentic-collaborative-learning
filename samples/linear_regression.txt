Linear Regression
=================

1. Definition and Theoretical Background
----------------------------------------
Linear regression models a continuous target \(y\) as a linear combination of input features \(x\). The simplest case, univariate linear regression, assumes:

\[
y = \beta_0 + \beta_1 x + \varepsilon,
\]

where \(\varepsilon\) is Gaussian noise. It seeks the line that best fits the data in a least‑squares sense.

2. Mathematical Derivations and Formulas
----------------------------------------
We determine \(\beta\) by minimizing the sum of squared errors:

\[
\min_{\beta_0,\beta_1} \sum_{i=1}^n (y_i - \beta_0 - \beta_1 x_i)^2.
\]

The closed‑form solution is:

\[
\beta_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}, 
\quad 
\beta_0 = \bar{y} - \beta_1 \bar{x}.
\]

3. Simple Example
-----------------
Data points:

| x | y   |
|---|-----|
| 1 | 1.5 |
| 2 | 2.0 |
| 3 | 2.5 |
| 4 | 4.0 |

Compute \(\bar{x}=2.5\), \(\bar{y}=2.5\). Then:

\[
\beta_1 = \frac{(1-2.5)(1.5-2.5)+\dots+(4-2.5)(4.0-2.5)}{\sum (x_i-2.5)^2} = 0.7,
\quad
\beta_0 = 2.5 - 0.7\times 2.5 = 0.75.
\]

Prediction for \(x=5\): \(y = 0.75 + 0.7 \times 5 = 4.25.\)

4. Detailed Understanding Check
-------------------------------
This summary explains the linear model, the least‑squares derivation, provides a numerical example, and checks that the reader can compute \(\beta\) and make predictions.  
